{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzIGwoteMVNw",
        "outputId": "3ddda8cb-f857-41aa-8ae8-2ebb8bf1437f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "wV6lqWl0MgtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator"
      ],
      "metadata": {
        "id": "hyGt8xyLNJlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "7KhUWjiVMovk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discriminator"
      ],
      "metadata": {
        "id": "kDq96L5uNQVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "XsO1rISnMo7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Setup"
      ],
      "metadata": {
        "id": "hQv5brCINYwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create the generator and discriminator\n",
        "netG = Generator().to(device)\n",
        "netD = Discriminator().to(device)\n",
        "\n",
        "# Initialize the weights\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)\n",
        "\n",
        "# Loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "latent_vector_size = 100\n",
        "\n",
        "# Data transformation for CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Function to convert RGB images to grayscale\n",
        "def rgb_to_grayscale(batch):\n",
        "    return batch.mean(dim=1, keepdim=True)\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we5mueamNSGB",
        "outputId": "b943e318-34ae-408f-fa80-7cf67415133f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 46563957.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "tgZrLBU9Nhkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs('output', exist_ok=True)\n"
      ],
      "metadata": {
        "id": "8XMbJWewPoW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # Update Discriminator\n",
        "        netD.zero_grad()\n",
        "        real_images, _ = data\n",
        "        real_images = real_images.to(device)\n",
        "\n",
        "        # Convert to grayscale\n",
        "        grayscale_images = rgb_to_grayscale(real_images)\n",
        "\n",
        "        batch_size = real_images.size(0)\n",
        "        labels = torch.full((batch_size,), 1, dtype=torch.float, device=device)\n",
        "\n",
        "        output = netD(real_images).view(-1)\n",
        "        errD_real = criterion(output, labels)\n",
        "        errD_real.backward()\n",
        "\n",
        "        noise = torch.randn(batch_size, latent_vector_size, 1, 1, device=device)\n",
        "        fake_images = netG(noise)\n",
        "        labels.fill_(0)\n",
        "        output = netD(fake_images.detach()).view(-1)\n",
        "        errD_fake = criterion(output, labels)\n",
        "        errD_fake.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        # Update Generator\n",
        "        netG.zero_grad()\n",
        "        labels.fill_(1)\n",
        "        output = netD(fake_images).view(-1)\n",
        "        errG = criterion(output, labels)\n",
        "        errG.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] Loss_D: {errD_real.item() + errD_fake.item()} Loss_G: {errG.item()}')\n",
        "\n",
        "    # Save fake images every epoch\n",
        "    save_image(fake_images, f'output/fake_images_epoch_{epoch}.png', normalize=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inkJmOPZNaui",
        "outputId": "ca7eb3d9-1ee9-4116-8bc7-2c69aadaff7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0/50][0/782] Loss_D: 0.6486326523590833 Loss_G: 8.115062713623047\n",
            "[0/50][100/782] Loss_D: 0.14724233746528625 Loss_G: 4.501054763793945\n",
            "[0/50][200/782] Loss_D: 0.24880269169807434 Loss_G: 4.623174667358398\n",
            "[0/50][300/782] Loss_D: 1.2991057224571705 Loss_G: 2.461794137954712\n",
            "[0/50][400/782] Loss_D: 0.7386796027421951 Loss_G: 2.32002854347229\n",
            "[0/50][500/782] Loss_D: 0.7644049376249313 Loss_G: 3.7227911949157715\n",
            "[0/50][600/782] Loss_D: 1.1464663445949554 Loss_G: 5.486776351928711\n",
            "[0/50][700/782] Loss_D: 0.9613341689109802 Loss_G: 1.838554859161377\n",
            "[1/50][0/782] Loss_D: 0.4510817676782608 Loss_G: 2.6202545166015625\n",
            "[1/50][100/782] Loss_D: 0.2887461483478546 Loss_G: 5.126065254211426\n",
            "[1/50][200/782] Loss_D: 0.47905173897743225 Loss_G: 3.712486505508423\n",
            "[1/50][300/782] Loss_D: 0.873783016577363 Loss_G: 5.537789344787598\n",
            "[1/50][400/782] Loss_D: 0.6930228769779205 Loss_G: 1.9276821613311768\n",
            "[1/50][500/782] Loss_D: 0.5091284960508347 Loss_G: 2.315997838973999\n",
            "[1/50][600/782] Loss_D: 0.7220286130905151 Loss_G: 3.074511766433716\n",
            "[1/50][700/782] Loss_D: 0.7853489965200424 Loss_G: 2.5585060119628906\n",
            "[2/50][0/782] Loss_D: 0.6948162019252777 Loss_G: 2.301835060119629\n",
            "[2/50][100/782] Loss_D: 0.4686989411711693 Loss_G: 2.738821029663086\n",
            "[2/50][200/782] Loss_D: 0.6427532732486725 Loss_G: 1.9788717031478882\n",
            "[2/50][300/782] Loss_D: 0.9059173911809921 Loss_G: 6.144243240356445\n",
            "[2/50][400/782] Loss_D: 1.015610545873642 Loss_G: 0.7214056253433228\n",
            "[2/50][500/782] Loss_D: 0.6081202477216721 Loss_G: 3.561187267303467\n",
            "[2/50][600/782] Loss_D: 0.5228652358055115 Loss_G: 2.0024726390838623\n",
            "[2/50][700/782] Loss_D: 0.460279643535614 Loss_G: 3.170588254928589\n",
            "[3/50][0/782] Loss_D: 0.33911924809217453 Loss_G: 4.217759132385254\n",
            "[3/50][100/782] Loss_D: 0.8904998227953911 Loss_G: 1.7491785287857056\n",
            "[3/50][200/782] Loss_D: 0.9301630854606628 Loss_G: 2.8393497467041016\n",
            "[3/50][300/782] Loss_D: 0.3072271794080734 Loss_G: 4.0328755378723145\n",
            "[3/50][400/782] Loss_D: 0.5207671858370304 Loss_G: 4.142907619476318\n",
            "[3/50][500/782] Loss_D: 1.6489718556404114 Loss_G: 1.4281827211380005\n",
            "[3/50][600/782] Loss_D: 0.20217252522706985 Loss_G: 3.418687343597412\n",
            "[3/50][700/782] Loss_D: 0.7816885411739349 Loss_G: 1.3266701698303223\n",
            "[4/50][0/782] Loss_D: 0.17443175241351128 Loss_G: 3.9542617797851562\n",
            "[4/50][100/782] Loss_D: 0.344472199678421 Loss_G: 2.92775821685791\n",
            "[4/50][200/782] Loss_D: 0.19821100682020187 Loss_G: 3.2641334533691406\n",
            "[4/50][300/782] Loss_D: 0.7143168151378632 Loss_G: 1.7869529724121094\n",
            "[4/50][400/782] Loss_D: 0.3106948360800743 Loss_G: 3.0543136596679688\n",
            "[4/50][500/782] Loss_D: 0.8517783805727959 Loss_G: 4.465649604797363\n",
            "[4/50][600/782] Loss_D: 0.3106086477637291 Loss_G: 3.0027666091918945\n",
            "[4/50][700/782] Loss_D: 0.20316216349601746 Loss_G: 3.5077528953552246\n",
            "[5/50][0/782] Loss_D: 0.13906706497073174 Loss_G: 4.101408004760742\n",
            "[5/50][100/782] Loss_D: 0.27067122608423233 Loss_G: 4.36325740814209\n",
            "[5/50][200/782] Loss_D: 1.0872976444661617 Loss_G: 5.7249860763549805\n",
            "[5/50][300/782] Loss_D: 0.41376854479312897 Loss_G: 2.867844581604004\n",
            "[5/50][400/782] Loss_D: 0.07616348192095757 Loss_G: 3.944622039794922\n",
            "[5/50][500/782] Loss_D: 0.8697016537189484 Loss_G: 2.4895243644714355\n",
            "[5/50][600/782] Loss_D: 0.6756154596805573 Loss_G: 3.057310104370117\n",
            "[5/50][700/782] Loss_D: 0.05151762068271637 Loss_G: 4.662888050079346\n",
            "[6/50][0/782] Loss_D: 0.8966267704963684 Loss_G: 3.211068630218506\n",
            "[6/50][100/782] Loss_D: 0.10910925269126892 Loss_G: 4.297715187072754\n",
            "[6/50][200/782] Loss_D: 0.38211481273174286 Loss_G: 2.467792272567749\n",
            "[6/50][300/782] Loss_D: 0.05338255316019058 Loss_G: 4.100106716156006\n",
            "[6/50][400/782] Loss_D: 0.7135685053654015 Loss_G: 5.777860641479492\n",
            "[6/50][500/782] Loss_D: 0.23229175992310047 Loss_G: 4.855231761932373\n",
            "[6/50][600/782] Loss_D: 0.5807057768106461 Loss_G: 2.711489200592041\n",
            "[6/50][700/782] Loss_D: 0.8736435174942017 Loss_G: 3.193301200866699\n",
            "[7/50][0/782] Loss_D: 0.08441989682614803 Loss_G: 3.6501524448394775\n",
            "[7/50][100/782] Loss_D: 0.4304269999265671 Loss_G: 3.5115435123443604\n",
            "[7/50][200/782] Loss_D: 0.26728224754333496 Loss_G: 2.264346122741699\n",
            "[7/50][300/782] Loss_D: 0.6995511204004288 Loss_G: 2.949420690536499\n",
            "[7/50][400/782] Loss_D: 0.06386155821383 Loss_G: 3.724637985229492\n",
            "[7/50][500/782] Loss_D: 0.475454218685627 Loss_G: 3.85545015335083\n",
            "[7/50][600/782] Loss_D: 0.7242044508457184 Loss_G: 1.8914340734481812\n",
            "[7/50][700/782] Loss_D: 0.45416995882987976 Loss_G: 3.2527761459350586\n",
            "[8/50][0/782] Loss_D: 0.04082304844632745 Loss_G: 4.90113639831543\n",
            "[8/50][100/782] Loss_D: 0.8418062925338745 Loss_G: 1.4300172328948975\n",
            "[8/50][200/782] Loss_D: 0.8469294905662537 Loss_G: 2.5082907676696777\n",
            "[8/50][300/782] Loss_D: 0.5182198584079742 Loss_G: 2.0641722679138184\n",
            "[8/50][400/782] Loss_D: 0.869071289896965 Loss_G: 2.397434711456299\n",
            "[8/50][500/782] Loss_D: 0.24265602976083755 Loss_G: 4.333975791931152\n",
            "[8/50][600/782] Loss_D: 0.6721997857093811 Loss_G: 2.213132619857788\n",
            "[8/50][700/782] Loss_D: 0.5296342372894287 Loss_G: 3.595643997192383\n",
            "[9/50][0/782] Loss_D: 0.2615707814693451 Loss_G: 2.905672073364258\n",
            "[9/50][100/782] Loss_D: 6.909145080872804 Loss_G: 4.848300933837891\n",
            "[9/50][200/782] Loss_D: 2.4605558677576482 Loss_G: 2.1548752784729004\n",
            "[9/50][300/782] Loss_D: 0.07810204662382603 Loss_G: 4.375992298126221\n",
            "[9/50][400/782] Loss_D: 0.4351654648780823 Loss_G: 2.4431021213531494\n",
            "[9/50][500/782] Loss_D: 0.08534594997763634 Loss_G: 4.287590503692627\n",
            "[9/50][600/782] Loss_D: 0.14176510646939278 Loss_G: 5.254930019378662\n",
            "[9/50][700/782] Loss_D: 0.9161887615919113 Loss_G: 4.600452423095703\n",
            "[10/50][0/782] Loss_D: 0.2190033495426178 Loss_G: 4.249595642089844\n",
            "[10/50][100/782] Loss_D: 0.6511164009571075 Loss_G: 2.5338540077209473\n",
            "[10/50][200/782] Loss_D: 0.07104151509702206 Loss_G: 4.574240684509277\n",
            "[10/50][300/782] Loss_D: 0.579586923122406 Loss_G: 3.9973762035369873\n",
            "[10/50][400/782] Loss_D: 0.29796628654003143 Loss_G: 3.3132405281066895\n",
            "[10/50][500/782] Loss_D: 0.48434123396873474 Loss_G: 2.6229734420776367\n",
            "[10/50][600/782] Loss_D: 0.48747995495796204 Loss_G: 3.1465303897857666\n",
            "[10/50][700/782] Loss_D: 0.7571300268173218 Loss_G: 2.082310199737549\n",
            "[11/50][0/782] Loss_D: 0.9130832552909851 Loss_G: 4.755221366882324\n",
            "[11/50][100/782] Loss_D: 0.06456275284290314 Loss_G: 3.9682841300964355\n",
            "[11/50][200/782] Loss_D: 0.07420996204018593 Loss_G: 4.292000770568848\n",
            "[11/50][300/782] Loss_D: 0.44446660578250885 Loss_G: 3.218071222305298\n",
            "[11/50][400/782] Loss_D: 0.012764863204210997 Loss_G: 5.772633075714111\n",
            "[11/50][500/782] Loss_D: 1.0776944160461426 Loss_G: 1.393263816833496\n",
            "[11/50][600/782] Loss_D: 0.8690005652606487 Loss_G: 5.717951774597168\n",
            "[11/50][700/782] Loss_D: 0.022273781709372997 Loss_G: 5.992231369018555\n",
            "[12/50][0/782] Loss_D: 1.0259028896689415 Loss_G: 4.234889984130859\n",
            "[12/50][100/782] Loss_D: 0.6250040829181671 Loss_G: 1.6933057308197021\n",
            "[12/50][200/782] Loss_D: 0.5026754103600979 Loss_G: 4.109272003173828\n",
            "[12/50][300/782] Loss_D: 0.17637955024838448 Loss_G: 3.882129192352295\n",
            "[12/50][400/782] Loss_D: 0.04992890916764736 Loss_G: 4.1382527351379395\n",
            "[12/50][500/782] Loss_D: 1.1901388466358185 Loss_G: 1.83134126663208\n",
            "[12/50][600/782] Loss_D: 0.4546213075518608 Loss_G: 3.032228469848633\n",
            "[12/50][700/782] Loss_D: 0.11511259153485298 Loss_G: 5.127875328063965\n",
            "[13/50][0/782] Loss_D: 0.24388218857347965 Loss_G: 3.5682835578918457\n",
            "[13/50][100/782] Loss_D: 0.028870658949017525 Loss_G: 5.52365779876709\n",
            "[13/50][200/782] Loss_D: 0.07610218599438667 Loss_G: 4.156207084655762\n",
            "[13/50][300/782] Loss_D: 0.6004014313220978 Loss_G: 2.9513471126556396\n",
            "[13/50][400/782] Loss_D: 0.18680164963006973 Loss_G: 3.512402296066284\n",
            "[13/50][500/782] Loss_D: 1.117128774523735 Loss_G: 2.5371248722076416\n",
            "[13/50][600/782] Loss_D: 0.7314386814832687 Loss_G: 3.2501347064971924\n",
            "[13/50][700/782] Loss_D: 0.25495763309299946 Loss_G: 2.4820423126220703\n",
            "[14/50][0/782] Loss_D: 1.0695699006319046 Loss_G: 1.7999087572097778\n",
            "[14/50][100/782] Loss_D: 0.015118454117327929 Loss_G: 5.401785850524902\n",
            "[14/50][200/782] Loss_D: 0.6623831987380981 Loss_G: 2.2153162956237793\n",
            "[14/50][300/782] Loss_D: 0.7247085869312286 Loss_G: 1.7136237621307373\n",
            "[14/50][400/782] Loss_D: 0.04165434464812279 Loss_G: 5.639375686645508\n",
            "[14/50][500/782] Loss_D: 0.014498976292088628 Loss_G: 7.523612976074219\n",
            "[14/50][600/782] Loss_D: 0.3865276649594307 Loss_G: 3.5894956588745117\n",
            "[14/50][700/782] Loss_D: 0.287903256714344 Loss_G: 4.6150994300842285\n",
            "[15/50][0/782] Loss_D: 0.15438294177874923 Loss_G: 3.650216579437256\n",
            "[15/50][100/782] Loss_D: 0.8935514688491821 Loss_G: 1.1776387691497803\n",
            "[15/50][200/782] Loss_D: 0.06792279705405235 Loss_G: 5.509035110473633\n",
            "[15/50][300/782] Loss_D: 0.8368730992078781 Loss_G: 2.8956120014190674\n",
            "[15/50][400/782] Loss_D: 1.0999408960342407 Loss_G: 1.583802580833435\n",
            "[15/50][500/782] Loss_D: 0.7061171233654022 Loss_G: 1.9290361404418945\n",
            "[15/50][600/782] Loss_D: 0.8319244161248207 Loss_G: 1.9401236772537231\n",
            "[15/50][700/782] Loss_D: 0.047116030007600784 Loss_G: 4.401406288146973\n",
            "[16/50][0/782] Loss_D: 0.7768600583076477 Loss_G: 2.9510586261749268\n",
            "[16/50][100/782] Loss_D: 0.006528464844450355 Loss_G: 6.6101274490356445\n",
            "[16/50][200/782] Loss_D: 0.7970148921012878 Loss_G: 1.5548057556152344\n",
            "[16/50][300/782] Loss_D: 0.818850040435791 Loss_G: 2.4137442111968994\n",
            "[16/50][400/782] Loss_D: 0.17182514816522598 Loss_G: 3.040497064590454\n",
            "[16/50][500/782] Loss_D: 0.025651615113019943 Loss_G: 5.97487735748291\n",
            "[16/50][600/782] Loss_D: 0.6696594730019569 Loss_G: 3.151386260986328\n",
            "[16/50][700/782] Loss_D: 0.7861060500144958 Loss_G: 1.3135442733764648\n",
            "[17/50][0/782] Loss_D: 0.5773565769195557 Loss_G: 3.2037081718444824\n",
            "[17/50][100/782] Loss_D: 2.6706790584139526 Loss_G: 0.7004021406173706\n",
            "[17/50][200/782] Loss_D: 0.6068033576011658 Loss_G: 1.6797022819519043\n",
            "[17/50][300/782] Loss_D: 0.8012302219867706 Loss_G: 2.4881348609924316\n",
            "[17/50][400/782] Loss_D: 1.2409611940383911 Loss_G: 1.4030320644378662\n",
            "[17/50][500/782] Loss_D: 1.0583522766828537 Loss_G: 4.36846923828125\n",
            "[17/50][600/782] Loss_D: 2.0324166901409626 Loss_G: 4.0140886306762695\n",
            "[17/50][700/782] Loss_D: 0.4994598925113678 Loss_G: 2.822266101837158\n",
            "[18/50][0/782] Loss_D: 1.5088580045849085 Loss_G: 6.228111267089844\n",
            "[18/50][100/782] Loss_D: 0.23192410171031952 Loss_G: 3.7383460998535156\n",
            "[18/50][200/782] Loss_D: 1.033383784815669 Loss_G: 0.9658953547477722\n",
            "[18/50][300/782] Loss_D: 0.9910389259457588 Loss_G: 2.480860710144043\n",
            "[18/50][400/782] Loss_D: 0.8124438300728798 Loss_G: 3.551619052886963\n",
            "[18/50][500/782] Loss_D: 0.7175018936395645 Loss_G: 1.3752355575561523\n",
            "[18/50][600/782] Loss_D: 0.13109482172876596 Loss_G: 5.957729339599609\n",
            "[18/50][700/782] Loss_D: 0.39967232942581177 Loss_G: 2.1116676330566406\n",
            "[19/50][0/782] Loss_D: 0.5901459753513336 Loss_G: 2.8181965351104736\n",
            "[19/50][100/782] Loss_D: 0.025181367062032223 Loss_G: 4.93525505065918\n",
            "[19/50][200/782] Loss_D: 0.5418657213449478 Loss_G: 2.4307546615600586\n",
            "[19/50][300/782] Loss_D: 0.8690365552902222 Loss_G: 1.203200340270996\n",
            "[19/50][400/782] Loss_D: 0.0983520932495594 Loss_G: 4.462768077850342\n",
            "[19/50][500/782] Loss_D: 0.36095258593559265 Loss_G: 4.475451469421387\n",
            "[19/50][600/782] Loss_D: 0.028247667476534843 Loss_G: 5.1348466873168945\n",
            "[19/50][700/782] Loss_D: 0.5246671140193939 Loss_G: 2.106919765472412\n",
            "[20/50][0/782] Loss_D: 0.1737043377943337 Loss_G: 6.378537178039551\n",
            "[20/50][100/782] Loss_D: 0.6425168514251709 Loss_G: 4.56667423248291\n",
            "[20/50][200/782] Loss_D: 0.7469896227121353 Loss_G: 3.135808229446411\n",
            "[20/50][300/782] Loss_D: 0.5820724368095398 Loss_G: 1.2568615674972534\n",
            "[20/50][400/782] Loss_D: 0.44214049726724625 Loss_G: 3.588782787322998\n",
            "[20/50][500/782] Loss_D: 0.026842089369893074 Loss_G: 5.326578140258789\n",
            "[20/50][600/782] Loss_D: 0.8127610012888908 Loss_G: 3.9909651279449463\n",
            "[20/50][700/782] Loss_D: 0.034568553790450096 Loss_G: 5.255717754364014\n",
            "[21/50][0/782] Loss_D: 0.6371385455131531 Loss_G: 3.202688217163086\n",
            "[21/50][100/782] Loss_D: 0.7426768839359283 Loss_G: 2.4785022735595703\n",
            "[21/50][200/782] Loss_D: 1.264805719256401 Loss_G: 2.4610705375671387\n",
            "[21/50][300/782] Loss_D: 1.94253084436059 Loss_G: 4.418628692626953\n",
            "[21/50][400/782] Loss_D: 0.17648497968912125 Loss_G: 4.080507278442383\n",
            "[21/50][500/782] Loss_D: 0.06038209609687328 Loss_G: 4.21246337890625\n",
            "[21/50][600/782] Loss_D: 1.5816818475723267 Loss_G: 0.7131460309028625\n",
            "[21/50][700/782] Loss_D: 0.04135996289551258 Loss_G: 5.3755059242248535\n",
            "[22/50][0/782] Loss_D: 0.055610258132219315 Loss_G: 4.617459774017334\n",
            "[22/50][100/782] Loss_D: 0.8198868185281754 Loss_G: 1.9231559038162231\n",
            "[22/50][200/782] Loss_D: 0.5126143917441368 Loss_G: 2.085076093673706\n",
            "[22/50][300/782] Loss_D: 0.016948656179010868 Loss_G: 5.367725372314453\n",
            "[22/50][400/782] Loss_D: 0.41633936762809753 Loss_G: 2.9933736324310303\n",
            "[22/50][500/782] Loss_D: 0.054705291986465454 Loss_G: 4.262531280517578\n",
            "[22/50][600/782] Loss_D: 0.48690637946128845 Loss_G: 2.636240243911743\n",
            "[22/50][700/782] Loss_D: 0.053550200536847115 Loss_G: 4.545729160308838\n",
            "[23/50][0/782] Loss_D: 0.46372190117836 Loss_G: 2.459744691848755\n",
            "[23/50][100/782] Loss_D: 0.10229288972914219 Loss_G: 4.6430511474609375\n",
            "[23/50][200/782] Loss_D: 0.34658004343509674 Loss_G: 3.353203773498535\n",
            "[23/50][300/782] Loss_D: 0.007091601844877005 Loss_G: 6.418290615081787\n",
            "[23/50][400/782] Loss_D: 0.6925206407904625 Loss_G: 5.221270561218262\n",
            "[23/50][500/782] Loss_D: 1.0084802508354187 Loss_G: 2.1370162963867188\n",
            "[23/50][600/782] Loss_D: 0.44891588389873505 Loss_G: 3.289423942565918\n",
            "[23/50][700/782] Loss_D: 0.47374704480171204 Loss_G: 3.060588836669922\n",
            "[24/50][0/782] Loss_D: 1.2862275838851929 Loss_G: 3.645205020904541\n",
            "[24/50][100/782] Loss_D: 0.30661703646183014 Loss_G: 3.273667335510254\n",
            "[24/50][200/782] Loss_D: 0.005893275141716003 Loss_G: 6.347822189331055\n",
            "[24/50][300/782] Loss_D: 0.28251083195209503 Loss_G: 3.5702242851257324\n",
            "[24/50][400/782] Loss_D: 0.12538166344165802 Loss_G: 4.438209533691406\n",
            "[24/50][500/782] Loss_D: 0.38652077317237854 Loss_G: 2.5289978981018066\n",
            "[24/50][600/782] Loss_D: 0.36906126141548157 Loss_G: 2.2405142784118652\n",
            "[24/50][700/782] Loss_D: 0.33792538195848465 Loss_G: 3.705320358276367\n",
            "[25/50][0/782] Loss_D: 0.037839394179172814 Loss_G: 7.1288557052612305\n",
            "[25/50][100/782] Loss_D: 0.9503833912312984 Loss_G: 5.246204376220703\n",
            "[25/50][200/782] Loss_D: 0.6984886825084686 Loss_G: 2.0340263843536377\n",
            "[25/50][300/782] Loss_D: 0.449847087264061 Loss_G: 2.833281993865967\n",
            "[25/50][400/782] Loss_D: 0.044080983847379684 Loss_G: 4.688533782958984\n",
            "[25/50][500/782] Loss_D: 0.6061275228857994 Loss_G: 4.241693019866943\n",
            "[25/50][600/782] Loss_D: 0.038155447691679 Loss_G: 5.903817653656006\n",
            "[25/50][700/782] Loss_D: 0.018213180359452963 Loss_G: 4.995515823364258\n",
            "[26/50][0/782] Loss_D: 0.022841671481728554 Loss_G: 5.196295261383057\n",
            "[26/50][100/782] Loss_D: 0.16093728691339493 Loss_G: 3.4648122787475586\n",
            "[26/50][200/782] Loss_D: 0.04005594179034233 Loss_G: 7.229833602905273\n",
            "[26/50][300/782] Loss_D: 0.7422878323122859 Loss_G: 6.242488861083984\n",
            "[26/50][400/782] Loss_D: 0.8415350783616304 Loss_G: 2.5862557888031006\n",
            "[26/50][500/782] Loss_D: 1.551025077700615 Loss_G: 4.988251686096191\n",
            "[26/50][600/782] Loss_D: 0.48677413910627365 Loss_G: 2.7256643772125244\n",
            "[26/50][700/782] Loss_D: 0.4644395411014557 Loss_G: 2.9544782638549805\n",
            "[27/50][0/782] Loss_D: 0.09828487038612366 Loss_G: 4.849923133850098\n",
            "[27/50][100/782] Loss_D: 0.9666183590888977 Loss_G: 1.8671905994415283\n",
            "[27/50][200/782] Loss_D: 0.41234147548675537 Loss_G: 2.947462320327759\n",
            "[27/50][300/782] Loss_D: 0.47766779363155365 Loss_G: 2.5226449966430664\n",
            "[27/50][400/782] Loss_D: 1.2662543803453445 Loss_G: 1.3956623077392578\n",
            "[27/50][500/782] Loss_D: 0.013127036392688751 Loss_G: 5.8084869384765625\n",
            "[27/50][600/782] Loss_D: 0.03461781470105052 Loss_G: 6.234579086303711\n",
            "[27/50][700/782] Loss_D: 0.0229790061712265 Loss_G: 5.339624404907227\n",
            "[28/50][0/782] Loss_D: 0.6572761088609695 Loss_G: 3.348036766052246\n",
            "[28/50][100/782] Loss_D: 0.37324003875255585 Loss_G: 2.636465311050415\n",
            "[28/50][200/782] Loss_D: 0.49878622591495514 Loss_G: 2.0608487129211426\n",
            "[28/50][300/782] Loss_D: 0.7589669823646545 Loss_G: 2.3818905353546143\n",
            "[28/50][400/782] Loss_D: 0.10514767095446587 Loss_G: 4.927402019500732\n",
            "[28/50][500/782] Loss_D: 0.02558123879134655 Loss_G: 5.297876358032227\n",
            "[28/50][600/782] Loss_D: 0.9939622282981873 Loss_G: 0.052541062235832214\n",
            "[28/50][700/782] Loss_D: 0.9501070380210876 Loss_G: 0.8037587404251099\n",
            "[29/50][0/782] Loss_D: 0.25791891664266586 Loss_G: 4.645179748535156\n",
            "[29/50][100/782] Loss_D: 0.4458325207233429 Loss_G: 3.815640926361084\n",
            "[29/50][200/782] Loss_D: 0.057437571696937084 Loss_G: 5.596029281616211\n",
            "[29/50][300/782] Loss_D: 0.005561319179832935 Loss_G: 6.781238555908203\n",
            "[29/50][400/782] Loss_D: 0.7552850842475891 Loss_G: 1.6462905406951904\n",
            "[29/50][500/782] Loss_D: 1.6927568432874978 Loss_G: 9.582207679748535\n",
            "[29/50][600/782] Loss_D: 0.8017124235630035 Loss_G: 2.993988037109375\n",
            "[29/50][700/782] Loss_D: 0.011874399613589048 Loss_G: 6.133447647094727\n",
            "[30/50][0/782] Loss_D: 0.7197964042425156 Loss_G: 1.2582132816314697\n",
            "[30/50][100/782] Loss_D: 0.003136177663691342 Loss_G: 7.353500843048096\n",
            "[30/50][200/782] Loss_D: 1.1291471906006336 Loss_G: 4.609249114990234\n",
            "[30/50][300/782] Loss_D: 0.019839940359815955 Loss_G: 5.65631103515625\n",
            "[30/50][400/782] Loss_D: 0.44808314740657806 Loss_G: 2.359389543533325\n",
            "[30/50][500/782] Loss_D: 0.6327522248029709 Loss_G: 2.8255863189697266\n",
            "[30/50][600/782] Loss_D: 0.2760867029428482 Loss_G: 3.0543675422668457\n",
            "[30/50][700/782] Loss_D: 0.4017469584941864 Loss_G: 2.5808303356170654\n",
            "[31/50][0/782] Loss_D: 0.04534795507788658 Loss_G: 4.836358547210693\n",
            "[31/50][100/782] Loss_D: 0.32996341586112976 Loss_G: 2.6546568870544434\n",
            "[31/50][200/782] Loss_D: 2.7198715154081583 Loss_G: 1.3899269104003906\n",
            "[31/50][300/782] Loss_D: 0.004006536211818457 Loss_G: 7.474733352661133\n",
            "[31/50][400/782] Loss_D: 0.15042489022016525 Loss_G: 3.7837934494018555\n",
            "[31/50][500/782] Loss_D: 0.009658469585701823 Loss_G: 7.248682498931885\n",
            "[31/50][600/782] Loss_D: 0.32189794816076756 Loss_G: 5.564621925354004\n",
            "[31/50][700/782] Loss_D: 0.019767656922340393 Loss_G: 5.36894416809082\n",
            "[32/50][0/782] Loss_D: 0.0016464113141410053 Loss_G: 7.318910598754883\n",
            "[32/50][100/782] Loss_D: 0.005740439984947443 Loss_G: 7.204965114593506\n",
            "[32/50][200/782] Loss_D: 0.004961940227076411 Loss_G: 7.14632511138916\n",
            "[32/50][300/782] Loss_D: 0.17178112268447876 Loss_G: 4.678615570068359\n",
            "[32/50][400/782] Loss_D: 0.019228733144700527 Loss_G: 5.226130485534668\n",
            "[32/50][500/782] Loss_D: 0.4202558398246765 Loss_G: 2.507333517074585\n",
            "[32/50][600/782] Loss_D: 0.7234615460038185 Loss_G: 1.1922574043273926\n",
            "[32/50][700/782] Loss_D: 0.5021280348300934 Loss_G: 1.7752817869186401\n",
            "[33/50][0/782] Loss_D: 0.25324375834316015 Loss_G: 4.724073886871338\n",
            "[33/50][100/782] Loss_D: 0.20611000806093216 Loss_G: 3.420156955718994\n",
            "[33/50][200/782] Loss_D: 0.5368574932217598 Loss_G: 4.129101276397705\n",
            "[33/50][300/782] Loss_D: 0.22292961180210114 Loss_G: 3.626572608947754\n",
            "[33/50][400/782] Loss_D: 0.0316674382193014 Loss_G: 7.201314926147461\n",
            "[33/50][500/782] Loss_D: 0.6434958949685097 Loss_G: 2.3663477897644043\n",
            "[33/50][600/782] Loss_D: 0.26560360193252563 Loss_G: 3.919288396835327\n",
            "[33/50][700/782] Loss_D: 1.4894299730658531 Loss_G: 5.697540283203125\n",
            "[34/50][0/782] Loss_D: 0.9229824747890234 Loss_G: 5.722892761230469\n",
            "[34/50][100/782] Loss_D: 0.5116788893938065 Loss_G: 1.3962697982788086\n",
            "[34/50][200/782] Loss_D: 0.855459600687027 Loss_G: 1.0639337301254272\n",
            "[34/50][300/782] Loss_D: 0.010305218049325049 Loss_G: 5.5116143226623535\n",
            "[34/50][400/782] Loss_D: 0.01342705124989152 Loss_G: 5.6393961906433105\n",
            "[34/50][500/782] Loss_D: 0.005342478980310261 Loss_G: 7.833737850189209\n",
            "[34/50][600/782] Loss_D: 0.004226025776006281 Loss_G: 6.717539310455322\n",
            "[34/50][700/782] Loss_D: 0.005629842053167522 Loss_G: 6.489613056182861\n",
            "[35/50][0/782] Loss_D: 0.0022792375821154565 Loss_G: 7.322822570800781\n",
            "[35/50][100/782] Loss_D: 0.004078469297382981 Loss_G: 7.205087661743164\n",
            "[35/50][200/782] Loss_D: 0.00013492219295585528 Loss_G: 10.453351974487305\n",
            "[35/50][300/782] Loss_D: 0.0055894136894494295 Loss_G: 7.706954002380371\n",
            "[35/50][400/782] Loss_D: 0.0008797632181085646 Loss_G: 7.752349853515625\n",
            "[35/50][500/782] Loss_D: 0.0004111056769033894 Loss_G: 8.951583862304688\n",
            "[35/50][600/782] Loss_D: 0.0008032323676161468 Loss_G: 8.956962585449219\n",
            "[35/50][700/782] Loss_D: 0.001448199531296268 Loss_G: 7.428478240966797\n",
            "[36/50][0/782] Loss_D: 0.0008788893392193131 Loss_G: 7.870475769042969\n",
            "[36/50][100/782] Loss_D: 0.0007759222644381225 Loss_G: 8.176773071289062\n",
            "[36/50][200/782] Loss_D: 0.0002534361046855338 Loss_G: 9.153244018554688\n",
            "[36/50][300/782] Loss_D: 0.00012102058099132072 Loss_G: 38.00663375854492\n",
            "[36/50][400/782] Loss_D: 2.4540930099303252e-05 Loss_G: 36.9405632019043\n",
            "[36/50][500/782] Loss_D: 9.446894182987177e-06 Loss_G: 36.252498626708984\n",
            "[36/50][600/782] Loss_D: 3.6880422599144163e-07 Loss_G: 35.479652404785156\n",
            "[36/50][700/782] Loss_D: 6.426164026749518e-07 Loss_G: 33.53434753417969\n",
            "[37/50][0/782] Loss_D: 0.01403095107525587 Loss_G: 55.408958435058594\n",
            "[37/50][100/782] Loss_D: 9.478850188315848e-06 Loss_G: 53.584964752197266\n",
            "[37/50][200/782] Loss_D: 0.00024794216733425856 Loss_G: 53.1268424987793\n",
            "[37/50][300/782] Loss_D: 8.956756573752822e-06 Loss_G: 51.6015739440918\n",
            "[37/50][400/782] Loss_D: 1.2517157301772391e-05 Loss_G: 52.535465240478516\n",
            "[37/50][500/782] Loss_D: 1.917507142934489e-05 Loss_G: 51.24793243408203\n",
            "[37/50][600/782] Loss_D: 5.15347119289832e-06 Loss_G: 51.20161056518555\n",
            "[37/50][700/782] Loss_D: 5.476194928633684e-07 Loss_G: 51.166786193847656\n",
            "[38/50][0/782] Loss_D: 6.89178989438374e-08 Loss_G: 50.743377685546875\n",
            "[38/50][100/782] Loss_D: 8.121269274852361e-07 Loss_G: 51.64204025268555\n",
            "[38/50][200/782] Loss_D: 4.830420948565011e-05 Loss_G: 51.521427154541016\n",
            "[38/50][300/782] Loss_D: 1.2610546491488142e-06 Loss_G: 51.12615966796875\n",
            "[38/50][400/782] Loss_D: 1.0874166036956033e-05 Loss_G: 51.25714111328125\n",
            "[38/50][500/782] Loss_D: 1.000246129478954e-06 Loss_G: 50.98542785644531\n",
            "[38/50][600/782] Loss_D: 1.0244557557943779e-07 Loss_G: 50.35483169555664\n",
            "[38/50][700/782] Loss_D: 3.330564140924286e-06 Loss_G: 50.273223876953125\n",
            "[39/50][0/782] Loss_D: 3.1275098990591963e-06 Loss_G: 50.42431640625\n",
            "[39/50][100/782] Loss_D: 1.4883153198754937e-06 Loss_G: 50.420745849609375\n",
            "[39/50][200/782] Loss_D: 1.980061142603541e-06 Loss_G: 50.525875091552734\n",
            "[39/50][300/782] Loss_D: 9.01541113762542e-07 Loss_G: 50.752994537353516\n",
            "[39/50][400/782] Loss_D: 1.8012253804046841e-06 Loss_G: 49.69865036010742\n",
            "[39/50][500/782] Loss_D: 8.065296697169972e-07 Loss_G: 49.872840881347656\n",
            "[39/50][600/782] Loss_D: 5.904628324064802e-07 Loss_G: 50.44371795654297\n",
            "[39/50][700/782] Loss_D: 1.1734673677220437e-07 Loss_G: 49.71312713623047\n",
            "[40/50][0/782] Loss_D: 4.4517517494591974e-07 Loss_G: 50.194252014160156\n",
            "[40/50][100/782] Loss_D: 4.070075192430635e-06 Loss_G: 49.70585632324219\n",
            "[40/50][200/782] Loss_D: 1.3038517821001522e-08 Loss_G: 49.723331451416016\n",
            "[40/50][300/782] Loss_D: 8.884884437066756e-07 Loss_G: 49.77284240722656\n",
            "[40/50][400/782] Loss_D: 2.5556439595677235e-06 Loss_G: 48.640323638916016\n",
            "[40/50][500/782] Loss_D: 5.1993652050452456e-06 Loss_G: 50.02789306640625\n",
            "[40/50][600/782] Loss_D: 4.792096660822557e-06 Loss_G: 49.52001953125\n",
            "[40/50][700/782] Loss_D: 1.5832499400454228e-07 Loss_G: 49.37892532348633\n",
            "[41/50][0/782] Loss_D: 2.4774647044981022e-06 Loss_G: 49.68256378173828\n",
            "[41/50][100/782] Loss_D: 9.049352229340102e-06 Loss_G: 49.70222854614258\n",
            "[41/50][200/782] Loss_D: 1.6763809895895898e-08 Loss_G: 49.348026275634766\n",
            "[41/50][300/782] Loss_D: 6.239911272134154e-07 Loss_G: 49.27655792236328\n",
            "[41/50][400/782] Loss_D: 1.6018782389443879e-07 Loss_G: 49.09245300292969\n",
            "[41/50][500/782] Loss_D: 1.0430819941132591e-07 Loss_G: 49.0418701171875\n",
            "[41/50][600/782] Loss_D: 2.5129038476748233e-06 Loss_G: 49.4476203918457\n",
            "[41/50][700/782] Loss_D: 5.587974669671227e-07 Loss_G: 48.99150085449219\n",
            "[42/50][0/782] Loss_D: 1.341106212786695e-07 Loss_G: 49.006797790527344\n",
            "[42/50][100/782] Loss_D: 1.460366206632049e-06 Loss_G: 49.51176834106445\n",
            "[42/50][200/782] Loss_D: 1.2479749500611944e-07 Loss_G: 49.188350677490234\n",
            "[42/50][300/782] Loss_D: 1.8812765745269951e-07 Loss_G: 49.168678283691406\n",
            "[42/50][400/782] Loss_D: 1.247973528967219e-07 Loss_G: 49.049957275390625\n",
            "[42/50][500/782] Loss_D: 4.805634148834304e-07 Loss_G: 49.00666809082031\n",
            "[42/50][600/782] Loss_D: 1.2666016857680612e-07 Loss_G: 48.52630615234375\n",
            "[42/50][700/782] Loss_D: 2.0489100200097877e-08 Loss_G: 48.577552795410156\n",
            "[43/50][0/782] Loss_D: 1.2983146007178593e-06 Loss_G: 48.5746955871582\n",
            "[43/50][100/782] Loss_D: 3.9115562347001106e-08 Loss_G: 48.769744873046875\n",
            "[43/50][200/782] Loss_D: 1.7322638257183382e-07 Loss_G: 48.62262725830078\n",
            "[43/50][300/782] Loss_D: 1.8253950884389486e-07 Loss_G: 48.269920349121094\n",
            "[43/50][400/782] Loss_D: 6.51925944366351e-08 Loss_G: 48.26008987426758\n",
            "[43/50][500/782] Loss_D: 5.029145100507544e-08 Loss_G: 48.48683547973633\n",
            "[43/50][600/782] Loss_D: 1.5087456972676528e-07 Loss_G: 48.38380432128906\n",
            "[43/50][700/782] Loss_D: 1.695008222872668e-07 Loss_G: 46.719749450683594\n",
            "[44/50][0/782] Loss_D: 5.401674485525995e-08 Loss_G: 47.85397720336914\n",
            "[44/50][100/782] Loss_D: 2.607703564277085e-08 Loss_G: 47.56314468383789\n",
            "[44/50][200/782] Loss_D: 1.862645506814446e-08 Loss_G: 45.99565505981445\n",
            "[44/50][300/782] Loss_D: 100.00000047683903 Loss_G: 0.0\n",
            "[44/50][400/782] Loss_D: 100.00000016018765 Loss_G: 0.0\n",
            "[44/50][500/782] Loss_D: 100.00000157022953 Loss_G: 0.0\n",
            "[44/50][600/782] Loss_D: 100.00000039488202 Loss_G: 0.0\n",
            "[44/50][700/782] Loss_D: 100.00000010803353 Loss_G: 0.0\n",
            "[45/50][0/782] Loss_D: 100.00000018253938 Loss_G: 0.0\n",
            "[45/50][100/782] Loss_D: 100.0000030437593 Loss_G: 0.0\n",
            "[45/50][200/782] Loss_D: 100.00000046193719 Loss_G: 0.0\n",
            "[45/50][300/782] Loss_D: 100.00000111573343 Loss_G: 0.0\n",
            "[45/50][400/782] Loss_D: 100.00000060350249 Loss_G: 0.0\n",
            "[45/50][500/782] Loss_D: 100.00000004284084 Loss_G: 0.0\n",
            "[45/50][600/782] Loss_D: 100.00000008381923 Loss_G: 0.0\n",
            "[45/50][700/782] Loss_D: 100.00000018626486 Loss_G: 0.0\n",
            "[46/50][0/782] Loss_D: 100.00000008381912 Loss_G: 0.0\n",
            "[46/50][100/782] Loss_D: 100.00000040047271 Loss_G: 0.0\n",
            "[46/50][200/782] Loss_D: 100.00000002980232 Loss_G: 0.0\n",
            "[46/50][300/782] Loss_D: 100.00000010244555 Loss_G: 0.0\n",
            "[46/50][400/782] Loss_D: 100.00000002980234 Loss_G: 0.0\n",
            "[46/50][500/782] Loss_D: 100.00000002235176 Loss_G: 0.0\n",
            "[46/50][600/782] Loss_D: 100.00000007078057 Loss_G: 0.0\n",
            "[46/50][700/782] Loss_D: 100.0000001043082 Loss_G: 0.0\n",
            "[47/50][0/782] Loss_D: 100.0000060060529 Loss_G: 0.0\n",
            "[47/50][100/782] Loss_D: 100.00000024773277 Loss_G: 0.0\n",
            "[47/50][200/782] Loss_D: 100.00000007450586 Loss_G: 0.0\n",
            "[47/50][300/782] Loss_D: 100.00000013597337 Loss_G: 0.0\n",
            "[47/50][400/782] Loss_D: 100.00000004284084 Loss_G: 0.0\n",
            "[47/50][500/782] Loss_D: 100.00000009126971 Loss_G: 0.0\n",
            "[47/50][600/782] Loss_D: 100.00000001303852 Loss_G: 0.0\n",
            "[47/50][700/782] Loss_D: 100.00000010989623 Loss_G: 0.0\n",
            "[48/50][0/782] Loss_D: 100.00000000745058 Loss_G: 0.0\n",
            "[48/50][100/782] Loss_D: 100.00000007264325 Loss_G: 0.0\n",
            "[48/50][200/782] Loss_D: 100.0 Loss_G: 0.0\n",
            "[48/50][300/782] Loss_D: 100.00000000186265 Loss_G: 0.0\n",
            "[48/50][400/782] Loss_D: 100.00000000372529 Loss_G: 0.0\n",
            "[48/50][500/782] Loss_D: 100.0000005457627 Loss_G: 0.0\n",
            "[48/50][600/782] Loss_D: 100.00000001862645 Loss_G: 0.0\n",
            "[48/50][700/782] Loss_D: 100.00000001117587 Loss_G: 0.0\n",
            "[49/50][0/782] Loss_D: 100.0 Loss_G: 0.0\n",
            "[49/50][100/782] Loss_D: 100.00000006146736 Loss_G: 0.0\n",
            "[49/50][200/782] Loss_D: 100.00000004842879 Loss_G: 0.0\n",
            "[49/50][300/782] Loss_D: 100.0000000167638 Loss_G: 0.0\n",
            "[49/50][400/782] Loss_D: 100.0000000167638 Loss_G: 0.0\n",
            "[49/50][500/782] Loss_D: 100.00000001862645 Loss_G: 0.0\n",
            "[49/50][600/782] Loss_D: 100.00000000745058 Loss_G: 0.0\n",
            "[49/50][700/782] Loss_D: 100.0000000167638 Loss_G: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Generator"
      ],
      "metadata": {
        "id": "9XgWitejRkLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    noise = torch.randn(64, latent_vector_size, 1, 1, device=device)\n",
        "    fake_images = netG(noise).detach().cpu()\n",
        "    save_image(fake_images, 'output/test_fake_images.png', normalize=True)\n"
      ],
      "metadata": {
        "id": "NHaRh1PiNj9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9glPGrRgeF1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5fqMovx6cL10"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}